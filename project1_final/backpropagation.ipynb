{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from tests_backpropagation import main_test\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "torch.set_default_dtype(torch.double)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class ``MyNet``\n",
    "\n",
    "Read carefully how ``MyNet`` is implemented in the cell below. In particular:  \n",
    "- ``n_hid`` is a list of integer, representing the number of hidden units in each hidden layer.   \n",
    "-  ``MyNet([2, 3, 2]) = MiniNet()`` where ``MiniNet`` is the neural network defined in the fourth tutorial, in which notations are also clarified.     \n",
    "- ``model.L`` is the number of hidden layers, ``L``   \n",
    "- ``model.f[l]`` is the activation function of layer ``l``, $f^{[l]}$ (here ``torch.tanh``)   \n",
    "- ``model.df[l]`` is the derivative of the activation function, $f'^{[l]}$   \n",
    "- ``model.a[l]``  is the tensor $A^{[l]}$, (shape: ``(1, n(l))``)   \n",
    "- ``model.z[l]``  is the tensor $Z^{[l]}$, (shape: ``(1, n(l))``)  \n",
    "- ``Weights $W^{[l]}$`` (shape: ``(n(l+1), n(l))``) and biases $\\mathbf{b}^{[l]}$ (shape: ``(n(l+1))``) can be accessed as follows:\n",
    "```\n",
    "weights = model.fc[str(l)].weight.data\n",
    "bias = model.fc[str(l)].bias.data\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNet(nn.Module):\n",
    "    def __init__(self, n_l = [2, 3, 2]):\n",
    "        super().__init__() \n",
    "        \n",
    "        \n",
    "        # number of layers in our network (following Andrew's notations)\n",
    "        self.L = len(n_l)-1\n",
    "        self.n_l = n_l\n",
    "        \n",
    "        # Where we will store our neuron values\n",
    "        # - z: before activation function \n",
    "        # - a: after activation function (a=f(z))\n",
    "        self.z = {i : None for i in range(1, self.L+1)}\n",
    "        self.a = {i : None for i in range(self.L+1)}\n",
    "\n",
    "        # Where we will store the gradients for our custom backpropagation algo\n",
    "        self.dL_dw = {i : None for i in range(1, self.L+1)}\n",
    "        self.dL_db = {i : None for i in range(1, self.L+1)}\n",
    "\n",
    "        # Our activation functions\n",
    "        self.f = {i : lambda x : torch.tanh(x) for i in range(1, self.L+1)}\n",
    "\n",
    "        # Derivatives of our activation functions\n",
    "        self.df = {\n",
    "            i : lambda x : (1 / (torch.cosh(x)**2)) \n",
    "            for i in range(1, self.L+1)\n",
    "        }\n",
    "        \n",
    "        # fully connected layers\n",
    "        # We have to use nn.ModuleDict and to use strings as keys here to \n",
    "        # respect pytorch requirements (otherwise, the model does not learn)\n",
    "        self.fc = nn.ModuleDict({str(i): None for i in range(1, self.L+1)})\n",
    "        for i in range(1, self.L+1):\n",
    "            self.fc[str(i)] = nn.Linear(in_features=n_l[i-1], out_features=n_l[i])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Input layer\n",
    "        self.a[0] = torch.flatten(x, 1)\n",
    "        \n",
    "        # Hidden layers until output layer\n",
    "        for i in range(1, self.L+1):\n",
    "\n",
    "            # fully connected layer\n",
    "            self.z[i] = self.fc[str(i)](self.a[i-1])\n",
    "            # activation\n",
    "            self.a[i] = self.f[i](self.z[i])\n",
    "\n",
    "        # return output\n",
    "        return self.a[self.L] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks\n",
    "\n",
    "Write a function ``backpropagation(model, y_true, y_pred)`` that computes:\n",
    "\n",
    "- $\\frac{\\partial L}{\\partial w^{[l]}_{i,j}}$ and store them in ``model.dL_dw[l][i,j]`` for $l \\in [1 .. L]$ \n",
    "- $\\frac{\\partial L}{\\partial b^{[l]}_{j}}$ and store them in ``model.dL_db[l][j]`` for $l \\in [1 .. L]$ \n",
    "\n",
    "assuming ``model`` is an instance of the ``MyNet`` class.\n",
    "\n",
    "A vectorized implementation would be appreciated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropagation(model, y_true, y_pred):\n",
    "    with torch.no_grad():\n",
    "\n",
    "        #layer L --> calculating BP1\n",
    "        dL_dy = 2*torch.sub(y_pred, y_true) #part one of BP1\n",
    "        tanh_prime_z_L = model.df[model.L](model.z[model.L]) #part two of BP2\n",
    "        dL_df_L = dL_dy * tanh_prime_z_L #completing BP1 for layer L \n",
    "        dL_df_k = dL_df_L #creating general for next layers (L-n) in backprop\n",
    "        \n",
    "        #dL_dw for layer L using BP4\n",
    "        #here we can use dL_df_k from above\n",
    "        h_k_T = model.a[model.L-1]\n",
    "        dL_dw = dL_df_L.T @ h_k_T\n",
    "        \n",
    "        #dL_db for layer L using BP3\n",
    "        #this is the same as our dL_df_L\n",
    "        dL_db = dL_df_L\n",
    "\n",
    "        #updating the weight and bias gradients\n",
    "        model.dL_dw[model.L] = dL_dw\n",
    "        model.dL_db[model.L] = dL_db.squeeze()\n",
    "        \n",
    "        for l in range(model.L-1, 0, -1):\n",
    "\n",
    "            #finding the derivative of the activation function for the layer\n",
    "            tanh_prime_z_l = model.df[l](model.z[l])\n",
    "\n",
    "            #dL_df_l --> using BP2 we need w^(l+1) and dL_df^(l+1)\n",
    "            #we already have w^(l+1) from forward pass and dL_df^(l+1) from the last round\n",
    "            #dL_df_k means layer l-1\n",
    "            w_from_last = model.fc[str(l+1)].weight.data\n",
    "            dL_df_k = (w_from_last.T @ dL_df_k.T).T * tanh_prime_z_l\n",
    "            \n",
    "            #dL_dw --> using BP4 we need activation from l-1 and dL_df_k \n",
    "            h_k_T = model.a[l-1]\n",
    "            dL_dw = dL_df_k.T @ h_k_T\n",
    "            \n",
    "            #dL_db --> using BP3 we only need dl_df_k\n",
    "            dL_db = dL_df_k\n",
    "            \n",
    "            #print(dL_db, torch.flatten(dL_db), torch.squeeze(dL_db), dL_db.squeeze())\n",
    "            #updating weights and bias gradients\n",
    "            model.dL_dw[l] = dL_dw\n",
    "            model.dL_db[l] = dL_db.squeeze()\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the cells below, and check the output\n",
    "\n",
    "- In the 1st cell, we use a toy dataset and the same architecture as the MiniNet class of the fourth tutorial. \n",
    "- In the 2nd cell, we use a few samples of the MNIST dataset with a consistent model architecture (``24x24`` black and white cropped images as input and ``10`` output classes). \n",
    "\n",
    "You can set ``verbose`` to ``True`` if you want more details about your computations versus what is expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " __________________________________________________________________ \n",
      "                          Check gradients                             \n",
      " __________________________________________________________________ \n",
      "\n",
      " TEST PASSED: Gradients consistent with autograd's computations.\n",
      "\n",
      " TEST PASSED: Gradients consistent with finite differences computations.\n",
      "\n",
      " __________________________________________________________________ \n",
      "                 Check that weights have been updated               \n",
      " __________________________________________________________________ \n",
      "\n",
      " TEST PASSED: Weights have been updated.\n",
      "\n",
      " __________________________________________________________________ \n",
      "                      Check computational graph                     \n",
      " __________________________________________________________________ \n",
      "\n",
      " TEST PASSED: All parameters seem correctly attached to the computational graph!\n",
      "\n",
      " __________________________________________________________________ \n",
      "                             Conclusion                     \n",
      " __________________________________________________________________ \n",
      "\n",
      " 4 / 4: ALL TEST PASSED :)\n"
     ]
    }
   ],
   "source": [
    "model = MyNet([2, 3, 2])\n",
    "main_test(backpropagation, model, verbose=False, data='toy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9912422/9912422 [00:00<00:00, 24883774.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST/raw/train-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28881/28881 [00:00<00:00, 51220166.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST/raw/train-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1648877/1648877 [00:00<00:00, 32169929.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<00:00, 10613107.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " __________________________________________________________________ \n",
      "                          Check gradients                             \n",
      " __________________________________________________________________ \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vebjornskre/opt/anaconda3/envs/INF265/lib/python3.10/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([1, 10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ====================== Epoch 1 ====================== \n",
      "\n",
      " -------- Gradcheck with finite differences  --------- \n",
      " residual error:\n",
      " [0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "\n",
      " --------- Comparing with autograd values  ----------- \n",
      "\n",
      " ******* fc['1'].weight.grad ******* \n",
      "  Our computation:\n",
      " tensor([[  1.7421e-27,   1.7421e-27,   1.7421e-27,  ...,   1.7421e-27,\n",
      "           1.7421e-27,   1.7421e-27],\n",
      "        [-5.9696e-177, -5.9696e-177, -5.9696e-177,  ..., -5.9696e-177,\n",
      "         -5.9696e-177, -5.9696e-177],\n",
      "        [ -3.7006e-19,  -3.7006e-19,  -3.7006e-19,  ...,  -3.7006e-19,\n",
      "          -3.7006e-19,  -3.7006e-19],\n",
      "        ...,\n",
      "        [-1.6763e-132, -1.6763e-132, -1.6763e-132,  ..., -1.6763e-132,\n",
      "         -1.6763e-132, -1.6763e-132],\n",
      "        [  1.4466e-40,   1.4466e-40,   1.4466e-40,  ...,   1.4466e-40,\n",
      "           1.4466e-40,   1.4466e-40],\n",
      "        [  9.5900e-56,   9.5900e-56,   9.5900e-56,  ...,   9.5900e-56,\n",
      "           9.5900e-56,   9.5900e-56]])\n",
      "\n",
      "  Autograd's computation:\n",
      " tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [-0., -0., -0.,  ..., -0., -0., -0.],\n",
      "        [-0., -0., -0.,  ..., -0., -0., -0.],\n",
      "        ...,\n",
      "        [-0., -0., -0.,  ..., -0., -0., -0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "\n",
      " ********* fc['1'].bias.grad ******* \n",
      "  Our computation:\n",
      " tensor([-4.1084e-27, 1.4078e-176,  8.7273e-19, -2.0038e+01,  2.7808e-77,\n",
      "         6.5821e-18, -3.4859e-43, -7.9225e-37, -3.4193e-25,  4.3084e-25,\n",
      "        -1.1987e-24, -2.1216e-34, -2.4377e-30, 3.9534e-132, -3.4116e-40,\n",
      "        -2.2617e-55])\n",
      "  Autograd's computation:\n",
      " tensor([  0.0000,   0.0000,   0.0000, -20.0377,   0.0000,   0.0000,   0.0000,\n",
      "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "          0.0000,   0.0000])\n",
      "\n",
      " ------------------- relative error ------------------ \n",
      "(fc[1].weight.grad, model.dL_dw[1]):   0.0000\n",
      "(fc[1].bias.grad,   model.dL_db[1]):   0.0000\n",
      "(fc[2].weight.grad, model.dL_dw[2]):   0.0000\n",
      "(fc[2].bias.grad,   model.dL_db[2]):   0.0000\n",
      "\n",
      " TEST PASSED: Gradients consistent with autograd's computations.\n",
      "\n",
      " TEST PASSED: Gradients consistent with finite differences computations.\n",
      "\n",
      " __________________________________________________________________ \n",
      "                 Check that weights have been updated               \n",
      " __________________________________________________________________ \n",
      "tensor([[-0.2101, -0.2493, -0.1720,  ..., -0.1747, -0.2023, -0.2544],\n",
      "        [ 0.6405,  0.6012,  0.6404,  ...,  0.6005,  0.6012,  0.6509],\n",
      "        [ 0.2595,  0.2065,  0.2728,  ...,  0.2550,  0.2008,  0.2776],\n",
      "        ...,\n",
      "        [ 0.4555,  0.4385,  0.5049,  ...,  0.4893,  0.4710,  0.4452],\n",
      "        [ 0.0960,  0.1214,  0.0961,  ...,  0.0811,  0.1427,  0.1286],\n",
      "        [-1.0650, -1.0229, -1.0307,  ..., -1.0668, -1.0503, -1.0247]])\n",
      "tensor([ 0.4872, -1.4792, -0.5399,  2.7849, -0.6366, -1.5495, -0.3878,  0.3257,\n",
      "         0.8094, -1.5389,  1.3450,  0.2497,  0.1751, -1.1013, -0.2663,  2.4902])\n",
      "\n",
      " TEST PASSED: Weights have been updated.\n",
      "\n",
      " __________________________________________________________________ \n",
      "                      Check computational graph                     \n",
      " __________________________________________________________________ \n",
      "\n",
      " TEST PASSED: All parameters seem correctly attached to the computational graph!\n",
      "\n",
      " __________________________________________________________________ \n",
      "                             Conclusion                     \n",
      " __________________________________________________________________ \n",
      "\n",
      " 4 / 4: ALL TEST PASSED :)\n"
     ]
    }
   ],
   "source": [
    "model = MyNet([24*24, 16, 10])\n",
    "main_test(backpropagation, model, verbose=True, data='mnist')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('nglm-env': conda)",
   "metadata": {
    "interpreter": {
     "hash": "ae36e8c2cbd9e14d80419493f2540eab6c211be174ac39ce04705a74740d0d8b"
    }
   },
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
